{"cells":[{"cell_type":"markdown","source":["<img src=\"https://databricks.com/wp-content/uploads/2018/10/spark-summit-19-black.png\" />"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) Overview\n\n<img width=\"300px\" src=\"https://docs.delta.io/latest/_static/delta-lake-logo.png\">\n\nThe core abstraction of the Delta Lake is an ACID compliant Spark Table.\n\n\n* Stored as PARQUET format in blob storage\n* ACID Transactions\n* Snapshot Isolation\n* Scalable Metadata management\n* Schema Enforcement\n* Time Travel"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) Scenario\n\nYou are a Data Engineer working for a company that processes data collected from many IoT devices.  You've been tasked to build an end-to-end pipeline to capture and process this data in near real-time (NRT).  You can think of a few creative ways to accomplish this using Apache Spark and open formats such as Parquet, but in the design process you've uncovered some challenges:\n\n\n* Many small files leading to bad downstream performance\n* Frequent changes to business logic, therefore data schema\n\nThe frequent changes to business logic is the scariest, as we may need to backfill the data multiple times.\n\nWe need a reliable way to update the old data as we are streaming the latest data. Having a pipeline that accommodates maximum flexibility would make our life much easier. Delta Lake's ACID guarantees and unified batch/streaming support make it a good fit."],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) Sample Dataset"],"metadata":{}},{"cell_type":"code","source":["PARQUET_PATH=\"/tmp/delta_tutorial/parquet_table\"\nDELTA_SILVER_PATH=\"/tmp/delta_tutorial/delta_table\"\nDELTA_GOLD_PATH=\"/tmp/delta_tutorial/delta_agg_table\"\n\n# Reset Env\ndbutils.fs.rm(PARQUET_PATH, True)\ndbutils.fs.rm(DELTA_SILVER_PATH, True)\ndbutils.fs.rm(DELTA_GOLD_PATH, True)\n\n# Make some configurations small-scale friendly\nsql(\"set spark.sql.shuffle.partitions = 1\")\nsql(\"set spark.databricks.delta.snapshotPartitions = 1\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) Using Parquet"],"metadata":{}},{"cell_type":"markdown","source":["##### Create a dataframe to read in our sample dataset"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import expr\nfrom pyspark.sql.types import *\n\nraw_data = spark.range(100000) \\\n  .selectExpr(\"if(id % 2 = 0, 'Open', 'Close') as action\") \\\n  .withColumn(\"date\", expr(\"cast(concat('2019-04-', cast(rand(5) * 30 as int) + 1) as date)\")) \\\n  .withColumn(\"device_id\", expr(\"cast(rand(5) * 100 as int)\"))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#####Write this out to parquet"],"metadata":{}},{"cell_type":"code","source":["raw_data.write.format(\"parquet\").partitionBy(\"date\").save(PARQUET_PATH)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display(spark.read.format(\"parquet\").load(PARQUET_PATH))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["display(spark.read.format(\"parquet\").load(PARQUET_PATH).groupBy(\"action\").count())"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["#####Append some data to our table"],"metadata":{}},{"cell_type":"code","source":["stream_data = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 100).load() \\\n  .selectExpr(\"'Open' as action\") \\\n  .withColumn(\"date\", expr(\"cast(concat('2019-04-', cast(rand(5) * 30 as int) + 1) as date)\")) \\\n  .withColumn(\"device_id\", expr(\"cast(rand(5) * 500 as int)\"))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["stream_data.writeStream.format(\"parquet\").partitionBy(\"date\").outputMode(\"append\") \\\n  .trigger(processingTime='5 seconds').option('checkpointLocation', PARQUET_PATH + \"/_chk\").start(PARQUET_PATH)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["display(spark.read.format(\"parquet\").load(PARQUET_PATH).groupBy(\"action\").count())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["\n\n\n\n\n\n\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Uh-oh. What happened to our Close actions?\n\n - You can't combine streaming and batch in the FileSink implementation in Apache Spark"],"metadata":{}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) Using Delta Lake"],"metadata":{}},{"cell_type":"markdown","source":["#####The syntax to create this table is almost identical, except we're using the format DELTA instead of PARQUET."],"metadata":{}},{"cell_type":"code","source":["raw_data.write.format(\"delta\").partitionBy(\"date\").save(DELTA_SILVER_PATH)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["display(spark.read.format(\"delta\").load(DELTA_SILVER_PATH).groupBy(\"action\").count())"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["stream_data.writeStream.format(\"delta\").partitionBy(\"date\").outputMode(\"append\") \\\n  .trigger(processingTime='5 seconds').option('checkpointLocation', DELTA_SILVER_PATH + \"/_chk\").start(DELTA_SILVER_PATH)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["display(spark.read.format(\"delta\").load(DELTA_SILVER_PATH).groupBy(\"action\").count())"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["#####Using our Delta Lake as a Source"],"metadata":{}},{"cell_type":"code","source":["delta_data_stream = spark.readStream \\\n  .option(\"maxFilesPerTrigger\", \"10\") \\\n  .format(\"delta\") \\\n  .load(DELTA_SILVER_PATH)\n  "],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["delta_data_stream.groupBy(\"action\", \"date\", \"device_id\") \\\n  .count() \\\n  .writeStream \\\n  .format(\"delta\") \\\n  .option(\"checkpointLocation\", DELTA_GOLD_PATH + \"/_checkpoint\") \\\n  .partitionBy(\"date\") \\\n  .outputMode(\"complete\") \\\n  .start(DELTA_GOLD_PATH)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["display(spark.read.format(\"delta\").load(DELTA_GOLD_PATH).orderBy(\"date\", \"device_id\", \"action\"))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) Schema Evolution"],"metadata":{}},{"cell_type":"code","source":["# Now we have more users, so let's add the user_id column to our table\nnew_data_with_new_col = spark.range(1000) \\\n  .selectExpr(\"'Open' as action\",\"cast(concat('2019-04-', cast(rand(5) * 3 as int) + 1) as date) as date\") \\\n  .withColumn(\"device_id\", expr(\"cast(rand(5) * 100 as int)\")) \\\n  .withColumn(\"user_id\", expr(\"cast(rand(10) * 100 as int)\"))\n  \nnew_data_with_new_col.write.format(\"delta\").partitionBy(\"date\").mode(\"append\").save(DELTA_SILVER_PATH)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Add the mergeSchema option\nnew_data_with_new_col.write.option(\"mergeSchema\",\"true\").format(\"delta\").partitionBy(\"date\").mode(\"append\").save(DELTA_SILVER_PATH)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) ACID Transactions"],"metadata":{}},{"cell_type":"markdown","source":["Let's go and take a look what happened to our streams"],"metadata":{}},{"cell_type":"code","source":["# Let's add the user_id column, changing some old partitions at a time\nspark.read.format(\"delta\").load(DELTA_SILVER_PATH) \\\n  .where(\"date <= '2019-05-01'\") \\\n  .withColumn(\"user_id\", expr(\"coalesce(user_id, cast(rand(10) * 100 as int))\")) \\\n  .repartition(30, \"date\") \\\n  .write.format(\"delta\").mode(\"overwrite\") \\\n  .option(\"replaceWhere\", \"date = '2019-05-01'\") \\\n  .save(DELTA_SILVER_PATH)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Delta doesn't support Hive style dynamic partition overwrites. It is bad practice, and leads to mistakes. Delta provides 'replaceWhere' to enforce data correctness."],"metadata":{}},{"cell_type":"code","source":["# Let's restart the stream with the addition of user_id\nspark.readStream \\\n  .option(\"maxFilesPerTrigger\", \"10\") \\\n  .format(\"delta\") \\\n  .load(DELTA_SILVER_PATH) \\\n  .groupBy(\"action\", \"date\", \"device_id\", \"user_id\") \\\n  .count() \\\n  .writeStream \\\n  .format(\"delta\") \\\n  .option(\"checkpointLocation\", DELTA_GOLD_PATH + \"/_checkpoint_new\") \\\n  .option(\"overwriteSchema\", \"true\") \\\n  .partitionBy(\"date\") \\\n  .outputMode(\"complete\") \\\n  .start(DELTA_GOLD_PATH)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["display(spark.read.format(\"delta\").load(DELTA_GOLD_PATH).orderBy(\"date\", \"device_id\", \"action\"))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) Time Travel"],"metadata":{}},{"cell_type":"markdown","source":["The transaction log is stored along with the data under the `_delta_log` directory."],"metadata":{}},{"cell_type":"code","source":["print(\"\\n\".join([f.name for f in dbutils.fs.ls(DELTA_SILVER_PATH + \"/_delta_log\") if f.name.endswith('json')]))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# latest version - 2, because in latest version - 1 we added the user_id column\nversion_before_schema_change = 10"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["display(spark.read.format(\"delta\").load(DELTA_SILVER_PATH))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["spark.read.format(\"delta\").load(DELTA_SILVER_PATH).count()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["display(spark.read.format(\"delta\").option(\"versionAsOf\", version_before_schema_change).load(DELTA_SILVER_PATH))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["spark.read.format(\"delta\").option(\"versionAsOf\", version_before_schema_change).load(DELTA_SILVER_PATH).count()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://kpistoropen.blob.core.windows.net/collateral/roadshow/logo_spark_tiny.png) Additional Topics & Resources\n* <a href=\"https://docs.delta.io/latest/index.html\" target=\"_blank\">Delta Lake Documentation</a>\n* <a href=\"https://delta.io\" target=\"_blank\">Delta Lake Website</a>"],"metadata":{}}],"metadata":{"name":"SAIS19: Delta Lake Tutorial","notebookId":8457045},"nbformat":4,"nbformat_minor":0}
